# CS336 Assignment 1 项目分析

## 项目概述

本项目实现了一个完整的自然语言处理(NLP)系统，包含：
1. **BPE分词器 (Byte Pair Encoding Tokenizer)** - 文本预处理
2. **Transformer架构** - 深度学习模型
3. **训练框架** - 模型训练和评估

---

## 第一部分：BPE分词器实现

### 1. BPE分词器的三层结构

```
输入文本
  ↓
[Pre-Tokenizer] 预分词器 - 初步分割文本
  ↓
[BPE Trainer] BPE训练器 - 学习字节对合并规则
  ↓
[BPE Tokenizer] BPE分词器 - 使用学到的规则编码文本
  ↓
Token IDs
```

### 1.1 Pre-Tokenizer (预分词器)

**文件**: `cs336_basics/bpe_tokenizer/pre_tokenizer.py`

**功能**: 将原始文本分割成单词级别的tokens

#### 核心概念:
- **特殊Token处理**: 某些特殊符号(如`<|endoftext|>`)需要保留为原子单位
- **正则表达式匹配**: 使用复杂的正则模式识别：
  - 英文单词和缩写('s, 'm, 'll等)
  - 数字
  - 标点符号
  - 空格

#### 关键类和方法:

```python
class PreTokenizer:
    def __init__(self, special_tokens: list[str])
        # 根据特殊token构建正则模式，保证不被拆分
        
    def pretokenize(self, text: str) -> List[bytes]:
        # 单次预分词：文本 → bytes列表
        # 例如: "hello world" → [b'hello', b' world']
        
    def pretokenize_iter(self, texts: Iterable[str]) -> Iterable[bytes]:
        # 迭代器版本，逐个产生bytes token
        
    def build_word_frequency(self, docs: Iterable[str]) -> Counter:
        # 统计每个word在语料库中的频率
        # 返回格式: {word_bytes: frequency}
        
    def read_corpus(self, input_path: str) -> Iterable[List[str]]:
        # 智能分块读取大文件，按特殊token分割语料库
        # 避免一次加载整个文件到内存
```

#### 技术亮点:
- **内存映射技术**: 使用`find_chunk_boundaries()`找到合适的文件断点
- **并行处理**: 将大文件分成100个chunk独立处理
- **流式迭代**: 边读边处理，适合大规模数据

---

### 1.2 BPE Trainer (BPE训练器)

**文件**: `cs336_basics/bpe_tokenizer/trainer.py`

**功能**: 学习BPE合并规则，构建词汇表

#### BPE算法原理:

```
步骤1: 初始化 - 将每个字节作为初始token
       "hello" → [h, e, l, l, o]

步骤2: 统计所有相邻字节对的频率
       (h,e):3次, (e,l):5次, (l,l):4次, (l,o):2次
       
步骤3: 选择频率最高的字节对进行合并
       合并(e,l) → "el"
       "hello" → [h, el, l, o]

步骤4: 重复步骤2-3，直到词汇表达到目标大小
       最终词汇表大小 = 256个字节 + num_merges合并操作
```

#### 关键实现:

```python
class BPETrainer:
    def __init__(self, vocab_size: int, special_tokens: List[str])
    
    def train(self, input_path: str) -> Tuple[Dict[int, bytes], List]:
        """
        步骤1: 读取语料库并统计词频
        步骤2: 初始化所有单字节和单词的分割
        步骤3: 迭代合并频率最高的字节对
        步骤4: 保存词汇表和合并规则
        """
        
    def initialize_splits_and_pairs(self, word_freqs: Counter):
        # 构建反向索引: pair → 包含该pair的所有words
        # 这样更新时只需处理受影响的word，提高效率
        
        self.splits[word] = [bytes([b]) for b in word]
        # word的字节表示: "cat" → [b'c', b'a', b't']
        
        self.pair_freqs[pair] = word_freq  # 统计pair在所有word中的总频率
        self.pair_to_words[pair] = {word1, word2, ...}  # 反向索引
```

#### 数据结构设计:

| 数据结构 | 用途 | 示例 |
|--------|------|------|
| `token_vocab` | ID → Token映射 | `{256: b'el', 257: b'lo', ...}` |
| `merges` | 记录每次合并的字节对 | `[(b'h', b'e'), (b'e', b'l'), ...]` |
| `splits` | 当前每个word的分割 | `{b'hello': [b'h', b'el', b'lo']}` |
| `pair_freqs` | 字节对的频率 | `{(b'h', b'e'): 150, ...}` |
| `pair_to_words` | 反向索引，加速更新 | `{(b'h', b'e'): {b'hello', b'help'}}` |
| `freq_max_heap` | 最大堆，快速找到频率最高的pair | 小顶堆存负数实现 |

#### 性能优化:

1. **堆优化** (最关键):
   - 问题: 每次都遍历所有pair找最大值 → O(n)
   - 解决: 使用最大堆 → O(log n)
   - 实现: Python的`heapq`是小顶堆，存储`(-freq, pair)`实现最大堆

   ```python
   def find_best_pairs(self) -> Tuple[bytes, bytes]:
       while self.freq_max_heap:
           neg_freq, pair = heapq.heappop(self.freq_max_heap)
           freq = -neg_freq
           # 验证pair仍然有效(懒惰删除策略)
           if pair in self.pair_freqs and self.pair_freqs[pair] == freq:
               return pair
   ```

2. **反向索引**:
   - 问题: 每次合并后需要更新所有word → O(总word数)
   - 解决: 用反向索引只更新受影响的word
   
   ```python
   affected_words = self.pair_to_words.get(best_pair, set())
   for word in affected_words:
       # 只更新包含best_pair的word
   ```

3. **特殊Token保护**:
   ```python
   # 特殊token被视为原子单位，不能参与合并
   if word in special_token_bytes:
       self.splits[word] = [word]  # 不拆分
   ```

---

### 1.3 BPE Tokenizer (分词器)

**文件**: `cs336_basics/bpe_tokenizer/tokenizer.py`

**功能**: 使用训练好的规则编码/解码文本

#### 编码过程:

```python
def calculate_token_ids(self, word: bytes) -> List[int]:
    """
    将word根据合并规则迭代分割，得到token ID序列
    
    例子: word = b'hello', 假设已知 b'el' 的ID为 256
    
    输入:  b'hello' → [b'h', b'e', b'l', b'l', b'o']
    循环1: 找最高频率的pair → (b'h', b'e')未被合并 (b'e', b'l') 被合并为256
           合并位置 = 1, 执行替换 → [b'h', b'el', b'l', b'o']
    循环2: 找最高频率的pair → (b'el', b'l') 或 (b'l', b'o')
    ...直到无法继续合并
    
    输出: 对应的ID序列
    """
    bytes_list = [bytes([b]) for b in word]  # 初始化为单字节
    
    while len(bytes_list) > 1:
        # 遍历所有相邻pair，找ID最小的(最先被合并的)
        for i, pair in enumerate(zip(bytes_list[:-1], bytes_list[1:])):
            idx = self.token_to_id.get(pair[0] + pair[1])
            if (idx is not None) and ((min_rule_idx is None) or (idx < min_rule_idx)):
                min_rule_idx = idx
                min_merge_pos = i
        
        if min_rule_idx is None:  # 没有pair可以合并
            break
        
        # 执行合并: [a, b, c] + 位置1 → [a, (b+c)]
        bytes_list[min_merge_pos:min_merge_pos + 2] = \
            [bytes_list[min_merge_pos] + bytes_list[min_merge_pos + 1]]
```

#### 完整编码流程:

```python
def encode(self, text: str) -> List[int]:
    # 步骤1: 预分词
    words = self.pre_tokenizer.pretokenize(text)
    # "Hello world!" → [b'Hello', b' world', b'!']
    
    # 步骤2: 对每个word计算token IDs
    ids = []
    for word in words:
        if word in self.token_to_id:  # word本身就是vocab中的token
            ids.append(self.token_to_id[word])
        elif word in self.word_to_ids:  # 缓存中有
            ids.extend(self.word_to_ids[word])
        else:  # 需要计算
            token_ids = self.calculate_token_ids(word)
            self.word_to_ids[word] = token_ids  # 缓存
            ids.extend(token_ids)
    
    return ids
```

#### 优化技巧:

1. **缓存机制**:
   ```python
   self.word_to_ids: Dict[bytes, List[int]] = {}
   # 缓存已计算的word → id序列映射
   # 避免重复计算相同的word
   ```

2. **两种模式**:
   - `encode()`: 一次性编码整个文本
   - `encode_iterable()`: 逐个token产生，适合流式处理

3. **大规模文件处理**:
   ```python
   def encode_to_npfile(self, input_path, output_path):
       # 逐行读取输入文件
       # 编码为numpy数组
       # 保存为.npy格式
   ```

---

## 第二部分：Transformer架构实现

### 2. Transformer核心组件

**文件**: `cs336_basics/transformer/module.py`

### 2.1 基础层 (Basic Layers)

#### 1. Linear 层 (线性变换)

```python
class Linear(nn.Module):
    def __init__(self, in_features: int, out_features: int):
        self.W = nn.Parameter(torch.empty((out_features, in_features)))
        # Xavier-like 初始化
        std = sqrt(2.0 / (in_features + out_features))
        torch.nn.init.trunc_normal_(self.W, mean=0, std=std, ...)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, ..., in_features)
        # W: (out_features, in_features)
        # output: (batch, ..., out_features)
        return einsum(x, self.W, 'batch ... input, output input -> batch ... output')
```

**特点**: 支持任意中间维度，使用einops库优化

#### 2. Embedding 层 (词嵌入)

```python
class Embedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        # num_embeddings: vocab_size (10000)
        # embedding_dim: d_model (512)
        self.weight: (vocab_size, d_model)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, seq_len) 包含token IDs
        # output: (batch, seq_len, d_model)
        return self.weight[x]  # 查表得到embedding
```

#### 3. RMSNorm 层 (层归一化)

```python
class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization"""
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # 计算均方根归一化
        # RMS(x) = sqrt(mean(x^2))
        mean_square = torch.mean(x ** 2, dim=-1, keepdim=True)
        rms = torch.sqrt(mean_square + eps)
        # 归一化并乘以可学习的缩放参数
        return (x / rms) * self.weight
```

**优势**: 比LayerNorm计算更快，效果相近

#### 4. SwiGLU 激活函数 (前馈网络)

```python
class SwiGLUFeedForward(nn.Module):
    """SWiGLU = (Linear1(x) * sigmoid(Linear1(x))) * Linear3(x)"""
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        w1_x = self.weight1(x)  # (batch, seq, d_ff)
        w3_x = self.weight3(x)  # (batch, seq, d_ff)
        
        # SiLU激活: x * sigmoid(x)
        silu = w1_x * torch.sigmoid(w1_x)
        
        # SwiGLU: SiLU(w1) * w3
        swiglu = silu * w3_x
        
        # 输出投影回d_model维度
        return self.weight2(swiglu)  # (batch, seq, d_model)
```

**特点**: 比传统ReLU更有表达力，使用了Gated线性单元

---

### 2.2 位置编码与注意力机制

#### 1. RoPE (旋转位置编码)

```python
class RoPE(nn.Module):
    """Rotary Position Embedding"""
    
    def _precompute_freqs_cis(self) -> torch.Tensor:
        """
        预计算旋转角度
        
        理论: 对于第m个位置, 第k个维度
        θ_k = base^(-2k/d) 其中 base=10000
        位置m的旋转角 = m * θ_k
        """
        freqs = 1.0 / (theta ** (torch.arange(0, d_k, 2) / d_k))
        # freqs shape: (d_k/2,)  例如d_k=64 → (32,)
        
        seq_idx = torch.arange(0, max_seq_len)
        # 外积: seq × d_k/2 → 每个位置的旋转频率
        freqs = einsum(seq_idx, freqs, "seq, d -> seq d")
        
        # 转换为复数形式: e^(iθ) = cos(θ) + i*sin(θ)
        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
        return freqs_cis  # (seq_len, d_k/2)
    
    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:
        """
        应用旋转位置编码到Q/K
        
        Q: (batch, head, seq, d_k) → 对偶数/奇数维度对进行旋转
        """
        # 将实数向量转为复数对 (x[2i], x[2i+1]) ↔ (x[2i] + i*x[2i+1])
        x_complex = torch.view_as_complex(x_real)
        
        # 乘以旋转: z' = z * e^(iθ)
        x_rotated = x_complex * rope[positions]
        
        # 转换回实数形式
        return torch.view_as_real(x_rotated)
```

**优势**: 
- 相对位置信息编码(旋转角度差)
- 长序列外推能力强
- 计算高效

#### 2. 缩放点积注意力 (Scaled Dot-Product Attention)

```python
def scaled_dot_product_attention(Q, K, V, mask=None) -> torch.Tensor:
    """
    标准的注意力机制
    
    Attention(Q,K,V) = softmax(Q*K^T / sqrt(d_k)) * V
    """
    # 步骤1: 计算注意力分数
    scores = Q @ K.T  # (batch, ..., seq_q, seq_k)
    
    # 步骤2: 缩放（防止梯度消失）
    scores /= sqrt(d_k)
    
    # 步骤3: 应用mask（因果mask防止看到未来token）
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -inf)
    
    # 步骤4: softmax归一化
    attn_weights = softmax(scores, dim=-1)
    
    # 步骤5: 加权求和
    output = attn_weights @ V  # (batch, ..., seq_q, d_v)
    
    return output
```

**因果mask例子**:
```
Position: 0 1 2 3
Token 0:  1 0 0 0  (只能看自己)
Token 1:  1 1 0 0  (能看token 0和1)
Token 2:  1 1 1 0  (能看token 0,1,2)
Token 3:  1 1 1 1  (能看所有token)
```

#### 3. 多头自注意力 (Multi-head Self-Attention)

```python
class MultiheadSelfAttention(nn.Module):
    """
    使用多个注意力头，各自独立学习不同的特征
    """
    
    def __init__(self, d_model: int, num_heads: int):
        self.d_k = d_model // num_heads  # 每个头的维度
        
        # 一次性计算所有head的Q,K,V
        self.w_qkv = Linear(d_model, d_model * 3)  # d_model → 3*d_model
        self.w_o = Linear(d_model, d_model)  # 输出投影
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, seq, d_model)
        
        # 步骤1: 计算Q,K,V并reshape
        QKV = self.w_qkv(x)  # (batch, seq, 3*d_model)
        Q, K, V = rearrange(QKV, 
            "batch seq (three head d_k) -> three batch head seq d_k",
            three=3, head=num_heads)
        # 每个: (batch, num_heads, seq, d_k)
        
        # 步骤2: 计算因果mask
        mask = torch.tril(torch.ones((seq, seq)))
        
        # 步骤3: 注意力计算
        attn_output = scaled_dot_product_attention(Q, K, V, mask)
        # (batch, num_heads, seq, d_k)
        
        # 步骤4: 合并所有头
        attn_output = rearrange(attn_output,
            "batch head seq d_k -> batch seq (head d_k)")
        # (batch, seq, d_model)
        
        # 步骤5: 输出投影
        return self.w_o(attn_output)
```

**多头可视化**:
```
输入 x: (batch, seq, 512)
  ↓
分成8个头，每个头处理64维
  ├─ Head 1: Attention on dims 0-63
  ├─ Head 2: Attention on dims 64-127
  ├─ ...
  └─ Head 8: Attention on dims 448-511
  ↓
8个输出合并 → (batch, seq, 512)
  ↓
输出投影 → (batch, seq, 512)
```

#### 4. 带RoPE的多头自注意力

```python
class MultiheadSelfAttentionWithRoPE(MultiheadSelfAttention):
    def __init__(self, d_model, num_heads, theta, max_seq_len):
        super().__init__(d_model, num_heads)
        self.rope = RoPE(theta, d_k, max_seq_len)
    
    def forward(self, x: torch.Tensor, token_positions: torch.Tensor):
        # ... Q,K,V计算同上 ...
        
        # 新增: 对Q和K应用RoPE位置编码
        Q = self.rope(Q, token_positions)
        K = self.rope(K, token_positions)
        
        # 其余流程相同
        return scaled_dot_product_attention(Q, K, V, mask)
```

---

### 2.3 Transformer块与完整模型

#### 1. Transformer块 (TransformerBlock)

```python
class TransformerBlock(nn.Module):
    """
    一个完整的Transformer块 = 自注意力 + 前馈网络
    
    采用 Pre-LN 架构：LN在输入，而不是输出
    """
    
    def __init__(self, d_model, num_heads, d_ff, ...):
        self.attention = MultiheadSelfAttentionWithRoPE(...)
        self.fnn = SwiGLUFeedForward(d_model, d_ff)
        self.norm1 = RMSNorm(d_model)
        self.norm2 = RMSNorm(d_model)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Pre-LN架构:
        
        输入 x
        ├─ [norm1] → 层归一化
        ├─ [attention] → 自注意力
        └─ [+ x] → 残差连接
           ↓ x2
        ├─ [norm2] → 层归一化
        ├─ [ffn] → 前馈网络
        └─ [+ x2] → 残差连接
           ↓ 输出
        """
        # 注意力分支
        attn_output = self.attention(
            self.norm1(x), 
            token_positions
        )
        x2 = x + attn_output  # 残差连接
        
        # 前馈分支
        ffn_output = self.fnn(self.norm2(x2))
        output = x2 + ffn_output  # 残差连接
        
        return output
```

**残差连接的作用**:
- 缓解梯度消失问题
- 使信息直接流向深层
- 加快收敛速度

#### 2. TransformerLM (语言模型)

```python
class TransformerLM(nn.Module):
    """完整的Transformer语言模型"""
    
    def __init__(self, vocab_size, context_length, num_layers, ...):
        # 词嵌入层: token_id → 向量
        self.token_embedding = Embedding(vocab_size=10000, embedding_dim=512)
        
        # 堆叠num_layers个Transformer块
        self.tf_blocks = nn.ModuleList([
            TransformerBlock(d_model=512, num_heads=16, ...)
            for _ in range(4)  # num_layers=4
        ])
        
        # 最终层归一化
        self.ln_final = RMSNorm(512)
        
        # 输出投影: d_model维 → vocab_size维
        self.output_embedding = Linear(512, 10000)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        前向传播流程:
        
        输入: (batch, seq_len) - token IDs
        
        ┌─ Token Embedding ──────────────┐
        │  (batch, seq) → (batch, seq, d_model)
        │
        ├─ TransformerBlock 1 ───────────┤
        │  (batch, seq, d_model) → (batch, seq, d_model)
        │
        ├─ TransformerBlock 2 ───────────┤
        │  ...
        │
        ├─ TransformerBlock 4 ───────────┤
        │
        ├─ Final LayerNorm ──────────────┤
        │  (batch, seq, d_model) → (batch, seq, d_model)
        │
        ├─ Output Projection ────────────┤
        │  (batch, seq, d_model) → (batch, seq, vocab_size)
        │
        └─ Logits ───────────────────────┘
        
        输出: (batch, seq_len, vocab_size) - logits
        """
        x = self.token_embedding(x)
        
        for block in self.tf_blocks:
            x = block(x)
        
        x = self.ln_final(x)
        x = self.output_embedding(x)
        
        return x
```

---

### 2.4 模型配置参数

```python
model_config = {
    "vocab_size": 10000,          # BPE词汇表大小
    "context_length": 256,        # 最大序列长度
    "num_layers": 4,              # Transformer块数
    "num_heads": 16,              # 多头注意力头数
    "d_model": 512,               # 模型隐藏维度
    "d_ff": 1344,                 # 前馈网络中间维度
    "rope_theta": 10000,          # RoPE基数
}
```

**模型大小计算**:
- Embedding参数: 10000 × 512 = 510万
- 每个Block:
  - Attention: 4个Linear × (512×512) ≈ 104万
  - FNN: 3个Linear × (512×1344) ≈ 206万
  - LayerNorm: 2×512 ≈ 1K
- 4个Block: (104+206)万 × 4 ≈ 1240万
- 输出层: 512×10000 ≈ 512万
- **总计**: ~2500万参数

---

## 第三部分：训练框架

### 3. 训练系统

**文件**: `cs336_basics/transformer/train.py`, `train_utils.py`

### 3.1 优化器实现

#### 1. AdamW优化器

```python
class AdamWOptimizer(torch.optim.Optimizer):
    """
    Adam with decoupled Weight Decay
    
    标准Adam: m_t = β1*m_{t-1} + (1-β1)*g_t
           v_t = β2*v_{t-1} + (1-β2)*g_t^2
           θ_t = θ_{t-1} - α*m̂_t / (√v̂_t + ε)
    
    AdamW改进: 权重衰减项独立计算
           θ_t = θ_{t-1} - α*m̂_t/(√v̂_t + ε) - λ*θ_{t-1}
    """
    
    def step(self):
        for param in params:
            # 初始化first/second moment
            state["exp_avg"] = β1*exp_avg + (1-β1)*grad
            state["exp_avg_sq"] = β2*exp_avg_sq + (1-β2)*grad^2
            
            # 偏差修正
            lr_t = lr * sqrt(1-β2^step) / (1-β1^step)
            
            # 参数更新
            param -= lr_t * exp_avg / (sqrt(exp_avg_sq) + eps)
            
            # 权重衰减 (解耦方式)
            param -= λ * param
```

#### 2. 学习率调度

```python
def learning_rate_cosine_schedule(
    it: int,
    max_learning_rate: float,
    min_learning_rate: float,
    warmup_iters: int,
    cosine_cycle_iters: int
) -> float:
    """
    学习率策略: Warmup + 余弦退火
    
    1. 预热阶段 (0 to warmup_iters):
       lr = max_lr * (it / warmup_iters)
       线性增加，让模型稳定开始训练
    
    2. 余弦衰减 (warmup_iters to cosine_cycle_iters):
       lr = min_lr + 0.5*(max_lr-min_lr)*(1+cos(π*t))
       平滑下降，最后保持在min_lr
    
    3. 常数 (after cosine_cycle_iters):
       lr = min_lr
       继续微调
    """
    if it < warmup_iters:
        return max_learning_rate * it / warmup_iters
    elif it < cosine_cycle_iters:
        cos_percent = (it - warmup_iters) / (cosine_cycle_iters - warmup_iters)
        return min_learning_rate + 0.5 * (max_learning_rate - min_learning_rate) * (
            1 + math.cos(math.pi * cos_percent)
        )
    else:
        return min_learning_rate
```

**可视化**:
```
学习率
  ↑
  |        ╱─────╲
max_lr ─ ╱         ╲___
  |    /               \
  |   /                 ────────
  | /
  |________________________________→ 迭代数
  0    warmup      cosine        end
```

#### 3. 梯度裁剪

```python
def clip_grad(params: Iterable[nn.Parameter], max_norm: float = 1.0):
    """
    防止梯度爆炸
    
    若 ||grad|| > max_norm，则 grad' = grad * max_norm/||grad||
    """
    # 计算总梯度范数
    total_norm = sum(p.grad.norm()**2 for p in params)**0.5
    
    # 计算裁剪系数
    clip_coef = max_norm / (total_norm + 1e-6)
    
    # 只在超出范围时裁剪
    if clip_coef < 1.0:
        for p in params:
            p.grad *= clip_coef
```

### 3.2 损失函数

```python
def cross_entropy(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
    """
    交叉熵损失: -log(softmax(logits)[target])
    
    数值稳定实现:
    L = -logits[target] + log(sum(exp(logits)))
    """
    # logits: (batch*seq_len, vocab_size)
    # targets: (batch*seq_len,)
    
    # 数值稳定的softmax (减去max)
    logits_stable = logits - logits.max(dim=-1, keepdim=True).values
    
    # 获取目标logit
    targets_logit = logits_stable.gather(1, targets.unsqueeze(1)).squeeze(1)
    
    # log(sum(exp))
    log_sum_exp = torch.log(torch.sum(torch.exp(logits_stable), dim=-1))
    
    # 损失 = -logits[target] + log_sum_exp
    loss = -targets_logit + log_sum_exp
    
    return loss.mean()
```

### 3.3 批次生成

```python
def get_batch(
    dataset: np.ndarray,
    batch_size: int,
    context_length: int,
    device: torch.device
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    从dataset随机抽取batch_size个样本
    每个样本包含context_length个token (context_length-1个输入+1个目标)
    """
    # 随机选择起始位置
    start_indices = np.random.randint(
        0, 
        len(dataset) - context_length, 
        size=batch_size
    )
    
    # 提取样本
    inputs = torch.stack([
        torch.from_numpy(dataset[i:i+context_length-1])
        for i in start_indices
    ])
    
    targets = torch.stack([
        torch.from_numpy(dataset[i+1:i+context_length])
        for i in start_indices
    ])
    
    return inputs.to(device), targets.to(device)
```

**例子**:
```
Dataset: [10, 24, 89, 45, 67, 12, 34, ...]

get_batch(batch_size=2, context_length=4):
  样本1: indices=0-2 → input=[10,24,89], target=[24,89,45]
  样本2: indices=3-5 → input=[45,67,12], target=[67,12,34]
```

### 3.4 模型评估

```python
def evaluate_model(
    model: nn.Module,
    dataset: np.ndarray,
    batch_size: int,
    context_length: int,
    num_batches: int
) -> float:
    """
    在验证集上评估，计算平均损失
    """
    model.eval()
    total_loss = 0.0
    
    with torch.no_grad():
        for _ in range(num_batches):
            inputs, targets = get_batch(...)
            logits = model(inputs)
            loss = cross_entropy(logits, targets)
            total_loss += loss.item()
    
    return total_loss / num_batches
```

### 3.5 检查点管理

```python
def save_checkpoint(model, optimizer, iteration, out_path):
    """保存模型和优化器状态"""
    torch.save({
        'model_state': model.state_dict(),
        'optimizer_state': optimizer.state_dict(),
        'iteration': iteration,
    }, out_path)

def load_checkpoint(path, model, optimizer):
    """加载检查点"""
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state'])
    optimizer.load_state_dict(checkpoint['optimizer_state'])
    return checkpoint['iteration']
```

---

## 第四部分：训练流程

### 4. 完整训练循环

```python
# 配置
total_steps = (total_tokens * total_epochs) // (batch_size * context_length)
# 例: (100000 * 0.5) / (16 * 256) ≈ 77步

for step in range(1, total_steps + 1):
    # 1. 清除梯度
    optimizer.zero_grad()
    
    # 2. 计算学习率
    lr_now = learning_rate_cosine_schedule(step, ...)
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr_now
    
    # 3. 获取批次
    inputs, targets = get_batch(training_dataset, ...)
    # inputs: (batch_size, context_length-1)
    # targets: (batch_size, context_length-1)
    
    # 4. 前向传播
    logits = model(inputs)  # (batch_size, seq_len, vocab_size)
    
    # 5. 计算损失
    loss = cross_entropy(logits, targets)
    
    # 6. 反向传播
    loss.backward()
    
    # 7. 记录梯度范数
    if step % log_freq == 0:
        grad_norm = compute_grad_norm(model.parameters())
    
    # 8. 梯度裁剪
    clip_grad(model.parameters(), max_norm=1.0)
    
    # 9. 优化器更新
    optimizer.step()
    
    # 10. 日志记录
    if step % log_freq == 0:
        print(f"Step {step}, Loss: {loss.item()}")
        wandb.log({"train_loss": loss.item(), "lr": lr_now})
    
    # 11. 验证
    if step % val_freq == 0:
        val_loss = evaluate_model(model, validation_dataset, ...)
        wandb.log({"val_loss": val_loss})
    
    # 12. 保存检查点
    if step % checkpoint_freq == 0:
        save_checkpoint(model, optimizer, step, f"checkpoint_{step}.pt")
```

---

## 第五部分：核心优化技术总结

### 5. 关键优化

| 技术 | 位置 | 作用 |
|------|------|------|
| **堆(Heap)** | BPE Trainer | 将pair查询从O(n)降低到O(logn) |
| **反向索引** | BPE Trainer | 更新时只处理受影响的word，加快合并 |
| **缓存** | BPE Tokenizer | 避免重复计算相同word的token IDs |
| **内存映射** | Pre-Tokenizer | 大文件分块处理，减少内存占用 |
| **RoPE** | Transformer | 相对位置编码，长序列外推强 |
| **Pre-LN** | Transformer | 在输入而非输出进行归一化，更稳定 |
| **SwiGLU** | Transformer | 比ReLU更有表达力的激活函数 |
| **余弦退火** | Training | 平滑的学习率调度，更好的收敛 |
| **梯度裁剪** | Training | 防止梯度爆炸 |
| **AdamW** | Training | 解耦权重衰减，更好的正则化 |

---

## 第六部分：数据流示意

### 6. 端到端数据流

```
原始文本文件
    ↓
[Pre-Tokenizer]
    ↓ (预分词)
"hello" → [b'h', b'e', b'l', b'l', b'o']
    ↓
[BPE Trainer] (可选，训练阶段)
    ↓ (学习合并规则)
vocab.bin, merges.bin
    ↓
[BPE Tokenizer]
    ↓ (编码)
Token IDs: [256, 257, 100, 45, ...]
    ↓ (保存为numpy)
token_ids.npy
    ↓
[训练数据加载]
    ↓ (批次采样)
inputs: (16, 255)    // 16个样本，255个输入token
targets: (16, 255)   // 对应的目标
    ↓
[TransformerLM]
    ↓ (前向传播)
logits: (16, 255, 10000)
    ↓ (损失计算)
loss: scalar
    ↓ (反向传播)
梯度更新
```

---

## 总结

本项目是一个完整的NLP系统实现，展示了：

1. **BPE分词器**: 从原始文本到token的完整流程
   - Pre-tokenizer: 初步分割
   - BPE Trainer: 学习高频字节对
   - BPE Tokenizer: 应用规则编码

2. **Transformer架构**: 现代深度学习的核心
   - 位置编码(RoPE)
   - 多头自注意力
   - SwiGLU前馈网络
   - 残差连接和层归一化

3. **训练系统**: 完整的优化和评估框架
   - AdamW优化器
   - 余弦退火学习率调度
   - 梯度裁剪
   - 检查点管理

关键特点：
- **高效**: 使用堆、反向索引等优化数据结构
- **可扩展**: 支持大规模数据和模型
- **现代**: 采用RoPE、Pre-LN等最新技术
- **稳定**: 完善的梯度管理和学习率调度

